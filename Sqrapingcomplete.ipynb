{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Dec 23 00:15:48 2018\n",
    "\n",
    "@author: noahten\n",
    "\"\"\"\n",
    "from reppy.robots import Robots\n",
    "import requests\n",
    "# rp = urllib.robotparser.RobotFileParser()\n",
    "# rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n",
    "# rp.read()\n",
    "# rrate = rp.request_rate(\"*\")\n",
    "# rrate.requests\n",
    "# rrate.seconds\n",
    "# rp.crawl_delay(\"*\")\n",
    "# rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\")\n",
    "# #False\n",
    "# rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n",
    "# #True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "AGENT_NAME = 'PyMOTW'\n",
    "URL_BASE = 'https://ja-jp.facebook.com//'\n",
    "parser = robotparser.RobotFileParser()\n",
    "parser.set_url(urllib.parse.urljoin(URL_BASE,'robots.txt'))\n",
    "parser.read()\n",
    "\n",
    "PATHS = [\n",
    "    '/',#ホームページのrobot\n",
    "    '/PyMOTW/',#ホームページの相対パスを書く\n",
    "    '/admin/',\n",
    "    '/downloads/',\n",
    "    ]\n",
    "\n",
    "for path in PATHS:\n",
    "    print ('%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path))\n",
    "    url = urllib.parse.urljoin(URL_BASE, path)\n",
    "    print ('%6s : %s' % (parser.can_fetch(AGENT_NAME, url), url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleMap から周辺情報を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQLに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pymysql.cursors\n",
    "import MySQLdb\n",
    "\n",
    "\n",
    "\n",
    "conn=MySQLdb.connect(host=\"localhost\",\n",
    "                          user='root',\n",
    "                          password='',\n",
    "                          db='1113mysql',\n",
    "                          charset='utf8',\n",
    "                    )\n",
    "\n",
    "\n",
    "url = 'https://api.flickr.com/services/rest/'\n",
    "API_KEY = '25b04bcb3cad7ed64fd7f278676dea3a'\n",
    "SECRET_KEY = 'e970d4a9b95367d5'\n",
    "page=5\n",
    "serchword='sky'\n",
    "\n",
    "query = {\n",
    "        'method': 'flickr.photos.search',\n",
    "        'api_key': API_KEY,\n",
    "        'text': serchword,  #検索ワード\n",
    "        'per_page': page,  #1ページ辺りのデータ数\n",
    "        'format': 'json',\n",
    "        'nojsoncallback': '1'\n",
    "        }\n",
    "\n",
    "r = requests.get(url, params=query)\n",
    "#ファイルへの書きこみ\n",
    "writer=json.dumps(r.json(), sort_keys=True, indent=2)\n",
    "f = open('flickr.json', 'w') \n",
    "f.writelines(writer) \n",
    "f.close()\n",
    "#ファイルの出力\n",
    "f=open(\"flickr.json\", \"r\",encoding=\"utf-8-sig\")\n",
    "js=json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "#https://farm{farm}.staticflickr.com/{server}/{id}_{secret}_{size}.jpg'\n",
    "\n",
    "s=\"URL: \\n\"\n",
    "for z in js[\"photos\"][\"photo\"]:\n",
    "    s +=\"https://farm\"+str(z[\"farm\"]) + \".staticflickr.com/\" + z[\"server\"] + \"/\" + z[\"id\"] + \"_\" + z[\"secret\"] + \"_b.jpg\"+ \"\\n\"\n",
    "\n",
    "\n",
    "urllist=(s.split('\\n'))\n",
    "urllist.pop(0)\n",
    "print(urllist)\n",
    "\n",
    "#カーソルの取得\n",
    "cur = conn.cursor()\n",
    "#データベースを作る\n",
    "cur.execute('DROP DATABASE IF EXISTS flickr')\n",
    "cur.execute('CREATE DATABASE flickr')\n",
    "#データベースを変える\n",
    "cur.execute('USE flickr')\n",
    "#テーブルの作成\n",
    "#execute()でSQL文を実行する\n",
    "cur.execute('DROP TABLE IF EXISTS items')\n",
    "cur.execute('''\n",
    "        CREATE TABLE items(\n",
    "        id integer,\n",
    "        URL text\n",
    "    )     \n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "#データの挿入\n",
    "#cur.execute('INSERT INTO items VALUES(%s,%s)', (1,'URL'))    \n",
    "\n",
    "cur.executemany('INSERT INTO items VAlUES (%(id)s, %(URL)s)',[\n",
    "                {'id':1,'URL':str(urllist[0])},\n",
    "                {'id':2,'URL':str(urllist[1])},\n",
    "                {'id':3,'URL':str(urllist[2])},\n",
    "                {'id':4,'URL':str(urllist[3])},\n",
    "                {'id':5,'URL':str(urllist[4])},\n",
    "                ])\n",
    "#commitで保存\n",
    "conn.commit()\n",
    "#データの抽出\n",
    "cur.execute('SELECT * FROM items')\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTubeに情報を送信PV取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import requests\n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from apiclient.discovery import build  # pip install google-api-python-cliet\n",
    "\n",
    "YOUTUBE_API_KEY = ['AIzaSyCYneLWyBSxGdJHxVuyBElhVK5mEnfcEjs']  # 環境変数からAPIキーを取得する。os.environ\n",
    "url='https://www.googleapis.com/auth/youtube.readonly'\n",
    "\n",
    "# YouTubeのAPIクライアントを組み立てる。build()関数の第1引数にはAPI名を、\n",
    "# 第2引数にはAPIのバージョンを指定し、キーワード引数developerKeyでAPIキーを指定する。\n",
    "# この関数は、内部的に https://www.googleapis.com/discovery/v1/apis/youtube/v3/rest という\n",
    "# URLにアクセスし、APIのリソースやメソッドの情報を取得する。\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "# キーワード引数で引数を指定し、search.listメソッドを呼び出す。\n",
    "# list()メソッドでgoogleapiclient.http.HttpRequestオブジェクトが得られ、\n",
    "# execute()メソッドを実行すると実際にHTTPリクエストが送られて、APIのレスポンスが得られる。\n",
    "query = youtube.search().list(\n",
    "    part='snippet',\n",
    "    q='ちはやふる',\n",
    "    type='video',\n",
    ").execute()\n",
    "\n",
    "#r = requests.get(url,params=query)\n",
    "#print(r)\n",
    "print(json.dumps(query,sort_keys=True , indent=4))\n",
    "#print (json.dumps(query.json(),sort_keys=True, indent=2))\n",
    "\n",
    "# search_responseはAPIのレスポンスのJSOqqNをパースしたdict。\n",
    "print('タイトル')\n",
    "for item in query['items']:\n",
    "    print(item['snippet']['title'])  # 動画のタイトルを表示する。\n",
    "print('概要欄')\n",
    "for description in query['items']:\n",
    "    print(description['snippet']['description']) #動画の概要欄を表示\n",
    "    \n",
    "print('チャンネルタイトル')\n",
    "for description in query['items']:\n",
    "    print(description['snippet']['channelTitle']) #動画の概要欄を表示\n",
    "    \n",
    "    if (description['snippet']['channelTitle'] == '東宝MOVIEチャンネル'):\n",
    "        print('映画へ')\n",
    "\n",
    "\n",
    "print('サムネイル')\n",
    "for thumbnails in query['items']:\n",
    "    print(thumbnails['snippet']['thumbnails']['default']['url']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "東宝　https://hlo.tohotheater.jp/net/movie/TNPI3090J01.do　から　入手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESTINY 鎌倉ものがたり', '劇場版総集編アニメ『刀剣乱舞-花丸-』～幕間回想録～', '火花', 'GODZILLA 怪獣惑星', 'ラストレシピ ～麒麟の舌の記憶～', 'ミックス。', 'ナラタージュ', '亜人', '映画 あさひなぐ', '忍びの国', '映画 妖怪ウォッチ シャドウサイド 鬼王の復活', '未成年だけどコドモじゃない', '嘘を愛する女', '祈りの幕が下りる時', '空海―KU-KAI―', '映画ドラえもん のび太の宝島', '坂道のアポロン', 'ちはやふる －結び－', '映画 クレヨンしんちゃん 爆盛！カンフーボーイズ ～拉麺大乱～', '名探偵コナン ゼロの執行人（しっこうにん）', 'いぬやしき', 'となりの怪物くん', 'ラプラスの魔女', 'のみとり侍', '恋は雨上がりのように', 'OVER DRIVE', '羊と鋼の森', '劇場版ポケットモンスター 2018', '未来のミライ', '劇場版 コード・ブルー-ドクターヘリ緊急救命-（仮）', '僕のヒーローアカデミアTHE MOVIE', '検察側の罪人', 'SUNNY 強い気持ち・強い愛', '累-かさね-', '響-HIBIKI-', '散り椿', '億男', 'マスカレード・ホテル', 'GODZILLA2（仮）']\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# アクセスするURL\n",
    "url = \"https://www.toho.co.jp/movie/index.html\"\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "# htmlをBeautifulSoupで扱う\n",
    "soup = BeautifulSoup(response, \"html.parser\")#lxml\n",
    "\n",
    "#for a in soup.find_all('a'):\n",
    "#    print(a.get('href'))\n",
    "\n",
    "#for a in soup.find_all('a'):\n",
    "#    print(a.get('class'))\n",
    "\n",
    "#title_tag=soup.title\n",
    "#title = title_tag.string\n",
    "\n",
    "#print(title)\n",
    "\n",
    "#CSSセレクタによってタイトルを取得\n",
    "#現在上映している映画の題名取得\n",
    "import lxml.html\n",
    "import requests\n",
    "info=[]\n",
    " \n",
    "url =\"https://www.toho.co.jp/movie/lineup/index.html\"\n",
    "target_html = requests.get(url).content\n",
    "film= lxml.html.fromstring(target_html)\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        info.append(film.cssselect('.thumb_ttl')[i].text_content())\n",
    "        i+=1\n",
    "    except:\n",
    "        break\n",
    "print(info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#fp=\"https://www.toho.co.jp/movie/lineup/index.html\"\n",
    "#soup=BeautifulSoup(fp,\"html.parse\")\n",
    "\n",
    "#CSSセレクタで選び出す\n",
    "#print(soup.select_one(\"li:nth-of-type(8)\").string)\n",
    "#print(soup.select_one(\"#ve-list > li:nth-of-type(4)\").string)\n",
    "#print(soup.select(\"#ve-list > li[data-lo='us']\")[1].string)\n",
    "#print(soup.select(\"#ve-list > li.black\")[1].string)\n",
    "\n",
    "#findメソッドで選び出す\n",
    "#cond={\"data-lo\":\"us\",\"class\":\"black\"}\n",
    "#print(soup.find(\"li\",cond).string)\n",
    "\n",
    "\n",
    "#findメソッドを２度組み合わせる\n",
    "#print(soup.find(id=\"ve-list\").find(\"li\",cond).string)\n",
    "\n",
    "\n",
    "\n",
    "#CSS\n",
    "\n",
    "#上映が近い映画の題名取得\n",
    "# import lxml.html\n",
    "# import requests\n",
    "# info2=[]\n",
    " \n",
    "#url =\"https://www.toho.co.jp/movie/lineup/index.html#coming_soon\"\n",
    "#target_html = requests.get(url).content\n",
    "#comfilm = lxml.html.fromstring(target_html)\n",
    "#i=0\n",
    "#while True:\n",
    "#     try:\n",
    "#         info2.append(comfilm.cssselect('.thumb_ttl')[i].text_content())\n",
    "#         i+=1\n",
    "#         if (comfilm.cssselect('.span')[0].text_content()=='COMING SOON'):\n",
    "#             break\n",
    "#     except:\n",
    "#         break\n",
    "# print(info2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
